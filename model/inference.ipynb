{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "451465c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpm import *\n",
    "from unet import Unet\n",
    "import wandb\n",
    "import config as cfg\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from preprocess import *\n",
    "\n",
    "def binarize(img):\n",
    "    b_img = img.astype(np.float32)\n",
    "    b_img = 255 * (b_img - np.min(b_img)) / (np.max(b_img) - np.min(b_img))\n",
    "    b_img = b_img.astype(np.uint8)\n",
    "\n",
    "    blured = cv2.GaussianBlur(b_img, create_kernel(b_img, 50), 0)\n",
    "\n",
    "    otsu_tr = threshold_otsu(blured) * 0.8\n",
    "    mask = np.where(blured >= otsu_tr, 1, 0).astype(np.uint8)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd8bfd6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_num = 7\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = f'{inf_num}'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# torch.cuda.set_device(cfg.inference_num)\n",
    "\n",
    "patches_artifact_dir = '/home/nikola.jovisic.ivi/exp_mammo/rsna/pipeline/artifacts/rsna_3c_256:v70'\n",
    "\n",
    "model_patch = Unet(dim=128, dim_mults=(1, 2, 2, 4, 4), channels=3)\n",
    "model_checkpoint = torch.load(f'{patches_artifact_dir}/model_131999.pt', map_location='cpu')\n",
    "model_patch.load_state_dict((dict([(n.replace('module.', ''), p) for n, p in model_checkpoint['model_state'].items()])))\n",
    "\n",
    "# samples_folder = '/lustre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c18a419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "(256, 256)\n"
     ]
    }
   ],
   "source": [
    "samples_folder = '/lustre/3c-anomaly'\n",
    "if not os.path.exists(samples_folder):\n",
    "    os.mkdir(samples_folder)\n",
    "        \n",
    "for i in range(14898):\n",
    "    if i % 8 == inf_num:\n",
    "        print(i)\n",
    "        with torch.no_grad():\n",
    "            save_dir = os.path.join(samples_folder, f'img_{str(i)}.png')\n",
    "            \n",
    "            if not os.path.exists(lc_dir):\n",
    "                os.makedirs(lc_dir)\n",
    "            if not os.path.exists(patches_dir):\n",
    "                os.makedirs(patches_dir)\n",
    "            \n",
    "            # load whole image\n",
    "            img = cv2.imread(f'/lustre/samples/rsna/v1/whole_images_original/img_{i}.png')\n",
    "            img = img[:, :, 0]/255\n",
    "            print(img.shape)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f76d977",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'local_context_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m inputs, black_idx \u001b[38;5;241m=\u001b[39m create_inputs(img_channels, img)\n\u001b[1;32m      7\u001b[0m batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(np\u001b[38;5;241m.\u001b[39marray(inputs))\n\u001b[0;32m----> 8\u001b[0m local_contexts \u001b[38;5;241m=\u001b[39m generate_patches(\u001b[43mlocal_context_model\u001b[49m, batch, device)\n\u001b[1;32m     10\u001b[0m lcl_img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((NUM_PATCHES\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m256\u001b[39m, NUM_PATCHES\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m256\u001b[39m))\n\u001b[1;32m     12\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'local_context_model' is not defined"
     ]
    }
   ],
   "source": [
    "            transform = Compose([ToTensor(), Normalize(img.mean(), img.std())])\n",
    "            img = transform(img)\n",
    "            \n",
    "            # generate local contexts\n",
    "            img_channels = create_channels_lcl_ctx(img)\n",
    "            inputs, black_idx = create_inputs(img_channels, img)\n",
    "            batch = torch.Tensor(np.array(inputs))\n",
    "            local_contexts = generate_patches(local_context_model, batch, device)\n",
    "            \n",
    "            lcl_img = np.zeros((NUM_PATCHES*256, NUM_PATCHES*256))\n",
    "\n",
    "            i = 0\n",
    "            j = 0\n",
    "            n = 0\n",
    "            num_rows = NUM_PATCHES\n",
    "            min_value = local_contexts.cpu().min()\n",
    "            max_value = local_contexts.cpu().max()\n",
    "            for idx in range(len(img_channels)):\n",
    "                if idx in black_idx:\n",
    "                    patch = torch.zeros((256, 256)) - abs(min_value)\n",
    "                else:\n",
    "                    patch = local_contexts[n]\n",
    "            #         patch = (patch - patch.min()) / (patch.max() - patch.min())\n",
    "                    patch = patch[0].cpu()\n",
    "                    n += 1\n",
    "            #     if i % 3 == 1 and j % 3 == 1:\n",
    "                lcl_img[j*256:(j+1)*256, i*256:(i+1)*256] = patch\n",
    "\n",
    "                i += 1\n",
    "                if i == num_rows:\n",
    "                    j += 1\n",
    "                    i = 0\n",
    "            \n",
    "            # generate patches\n",
    "            patch_inputs, black_idx_patches = create_patch_inputs_from_lcl_ctx_img(lcl_img, img)\n",
    "            \n",
    "            n = 0\n",
    "            patches_arr = []\n",
    "            while n < len(patch_inputs):\n",
    "                patch_batch = torch.Tensor(np.array(patch_inputs[n:n+40]))\n",
    "                patches = generate_patches(model_patch, patch_batch, device)\n",
    "                patches_arr.append(patches)\n",
    "                n += 40\n",
    "                \n",
    "            patches = torch.cat(patches_arr, dim=0)\n",
    "\n",
    "            # final image\n",
    "            lc_normalized = (local_contexts - local_contexts.min()) / (local_contexts.max() - local_contexts.min())\n",
    "            patches_normalized = (patches - patches.min()) / (patches.max() - patches.min())\n",
    "            \n",
    "            \n",
    "            num_patches = int(np.round(ORIG_IMG_REAL_SIZE/PATCH_REAL_SIZE))\n",
    "\n",
    "            final = np.zeros((num_patches*256, num_patches*256))\n",
    "            i = 0\n",
    "            j = 0\n",
    "            n = 0\n",
    "            num_rows = num_patches\n",
    "            for idx in range(num_patches*num_patches):\n",
    "                if idx in black_idx_patches:\n",
    "                    patch = torch.zeros((256, 256))\n",
    "                else:\n",
    "                    patch = patches_normalized[n]\n",
    "            #         patch = (patch - patch.min()) / (patch.max() - patch.min())\n",
    "                    patch = patch[0].cpu()\n",
    "                    n += 1\n",
    "            #     if i % 3 == 1 and j % 3 == 1:\n",
    "                final[j*256:(j+1)*256, i*256:(i+1)*256] = patch\n",
    "                i += 1\n",
    "                if i == num_rows:\n",
    "                    j += 1\n",
    "                    i = 0    \n",
    "                    \n",
    "            # save images\n",
    "            lcl_img = (lcl_img - lcl_img.min()) / (lcl_img.max() - lcl_img.min())\n",
    "\n",
    "            save_image(torch.tensor(final), str(Path(save_dir) / f'pipeline.png'))\n",
    "            save_image(torch.tensor(lcl_img), str(Path(save_dir) / f'local_contexts_img.png'))\n",
    "            for i, lc in enumerate(lc_normalized):\n",
    "                save_image(lc[0], str(Path(lc_dir) / f'local_context-{i}.png'))\n",
    "            for i, patch in enumerate(patches_normalized):\n",
    "                save_image(patch[0], str(Path(patches_dir) / f'patch-{i}.png'))\n",
    "                \n",
    "            mask = binarize(final)\n",
    "            mask = dilate(mask)\n",
    "            test = final*mask\n",
    "            save_image(torch.tensor(test), str(Path(save_dir) / f'processed.png'))\n",
    "\n",
    "            del local_contexts\n",
    "            del patches\n",
    "            torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ecd81a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
